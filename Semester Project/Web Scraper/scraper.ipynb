{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect all Course links from html page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Links to all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"SOC/Schedule of Classes.html\"\n",
    "with open(filename, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "pattern = r\"https://app.testudo.umd.edu/soc/202308/(.{4})\"\n",
    "matches = re.findall(pattern, content)\n",
    "\n",
    "urls = []\n",
    "for match in matches:\n",
    "    url = \"https://app.testudo.umd.edu/soc/202308/\" + match\n",
    "    urls.append(url)\n",
    "\n",
    "\n",
    "def scrape_class_ids(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    class_ids = []\n",
    "\n",
    "    for course in soup.find_all('div', {'class': 'course'}):\n",
    "        class_id = course.find('div', {'class': 'course-id'}).text.strip()\n",
    "        class_ids.append(class_id)\n",
    "\n",
    "    return class_ids\n",
    "\n",
    "\n",
    "url_class_id = []\n",
    "updated_urls  = [s.replace(\"202308\", \"202301\") for s in urls]\n",
    "\n",
    "for url in updated_urls:\n",
    "    class_ids = scrape_class_ids(url)\n",
    "    for class_id in class_ids:\n",
    "        new_url = url + '/' + class_id\n",
    "        url_class_id.append(new_url)\n",
    "\n",
    "\n",
    "updated_url_class_id  = [s.replace(\"202308\", \"202301\") for s in url_class_id]\n",
    "\n",
    "#print(updated_url_class_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract course information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_section_data(url):\n",
    "    #print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    html_content = str(soup)\n",
    "\n",
    "    section_data = []\n",
    "    if \"No courses matched your search filters above.\" in html_content or \\\n",
    "       \"Contact department or instructor for details.\" in html_content or \\\n",
    "       '<span class=\"building-code\">TBA</span>' in html_content or \\\n",
    "       '<span class=\"section-days\">TBA</span>' in html_content or \\\n",
    "       '<span class=\"class-start-time\">TBA</span>' in html_content or \\\n",
    "       '<span class=\"class-end-time\">TBA</span>' in html_content or \\\n",
    "       '<span class=\"section_id\">TBA</span>' in html_content or \\\n",
    "       '<span class=\"course_id\">TBA</span>' in html_content:\n",
    "        print(f\"{url} No courses found for \")\n",
    "        return []\n",
    "\n",
    "\n",
    "    for section in soup.find_all('div', {'class': 'section delivery-f2f'}):\n",
    "        section_id = section.find('span', {'class': 'section-id'}).text.strip()\n",
    "        total_seats_count = section.find('span', {'class': 'total-seats-count'}).text.strip()\n",
    "        open_seats_count = section.find('span', {'class': 'open-seats-count'}).text.strip()\n",
    "        section_days = section.find('span', {'class': 'section-days'}).text.strip()\n",
    "        class_start_time = section.find('span', {'class': 'class-start-time'}).text.strip()\n",
    "        class_end_time = section.find('span', {'class': 'class-end-time'}).text.strip()\n",
    "        class_building = section.find('span', {'class': 'class-building'}).text.strip()\n",
    "        class_room = section.find('span', {'class': 'class-room'}).text.strip()\n",
    "\n",
    "        section_data.append({\n",
    "        'course_id': url[-7:],\n",
    "        'section_id': section_id,\n",
    "        'total_seats_count': total_seats_count,\n",
    "        'open_seats_count': open_seats_count,\n",
    "        'section_days': section_days,\n",
    "        'class_start_time': class_start_time,\n",
    "        'class_end_time': class_end_time,\n",
    "        'class_building': class_building.replace('\\n', ''),\n",
    "        'class_room': class_room\n",
    "        })\n",
    "    return section_data\n",
    "    \n",
    "urls_testing = ['https://app.testudo.umd.edu/soc/202301/AASP/AASP255',\n",
    " 'https://app.testudo.umd.edu/soc/202308/AASP/AASP100H',\n",
    " 'https://app.testudo.umd.edu/soc/202308/AASP/AASP101']\n",
    "\n",
    "fieldnames = ['course_id', 'section_id', 'total_seats_count', 'open_seats_count', 'section_days', 'class_start_time', 'class_end_time', 'class_building', 'class_room']\n",
    "\n",
    "with open('section_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for url in updated_url_class_id:\n",
    "        section_data = scrape_section_data(url)\n",
    "        csv_writer.writerows(section_data)\n",
    "        #print(section_data)updated_url_class_id\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
